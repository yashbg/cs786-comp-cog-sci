{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "\\- Yash Gupta (190997)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-barbados",
   "metadata": {},
   "source": [
    "### Part 1: 25 marks\n",
    "\n",
    "Fill in the partially complete lines of code in the file temporal_context_model_empty.m to make a temporal context model of memory retrieval that retrieves about 7 items from an encoded list efficiently.\n",
    "\n",
    "This should not be very challenging if you've followed the concept of the TCM in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from black import out\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random as rand\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_a_dist(p):\n",
    "    if np.sum(p) == 0:\n",
    "        p = 0.05 * np.ones(len(p))\n",
    "    p /= np.sum(p)\n",
    "    idx = np.where(rand.random(1) - np.cumsum(p) < 0)\n",
    "    sample = np.min(idx)\n",
    "    out = np.zeros(len(p))\n",
    "    out[sample] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORLD_FEATURES = 5\n",
    "N_ITEMS = 10\n",
    "ENCODING_TIME = 500\n",
    "TEST_TIME = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = np.column_stack((np.sort(np.round(rand.random(N_ITEMS) * ENCODING_TIME)), np.arange(N_ITEMS)))\n",
    "schedule_load = ENCODING_TIME / np.median(np.diff(schedule[:, 0]))\n",
    "encoding = np.zeros((N_ITEMS, N_WORLD_FEATURES + 1))\n",
    "\n",
    "world_m = np.array([1, 2, 1, 2, 3], dtype=float)\n",
    "world_var = 1\n",
    "delta = 0.05\n",
    "beta_param = 0.001\n",
    "m = 0\n",
    "\n",
    "for time in range(ENCODING_TIME):\n",
    "    world_m += delta\n",
    "    world = rand.normal(world_m, world_var)\n",
    "    # \n",
    "    if m < N_ITEMS:\n",
    "        if time == schedule[m, 0]:\n",
    "            encoding[m, :] = np.append(world, m)\n",
    "            m += 1\n",
    "\n",
    "out = np.zeros(TEST_TIME)\n",
    "while time < ENCODING_TIME + TEST_TIME:\n",
    "    world_m += delta\n",
    "    world = rand.normal(world_m, world_var)\n",
    "    soa = np.zeros(N_ITEMS)\n",
    "    for m in range(N_ITEMS):\n",
    "        soa[m] = np.dot(encoding[m], np.append(world, m))\n",
    "    soa /= np.linalg.norm(soa)\n",
    "    out[time - ENCODING_TIME] = np.where(draw_from_a_dist(soa) > 0)[0]\n",
    "    time += 1\n",
    "\n",
    "success = len(np.unique(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-barbados",
   "metadata": {},
   "source": [
    "### Part 2: 25 marks\n",
    "\n",
    "Alongside the TCM you have implemented, you have also implemented assumptions about the physics of the world. Namely, you have assumed that the world can be represented as a set of draws from N independent Gaussians, and that the means of these Gaussians evolve over time linearly. Now we change this. We assume that the world contains context changes, and represent this fact by sampling the rate of drift over time (delta) in the feature means itself from a mixture of two Gaussians, one with a small mean to denote small changes, and one with a large mean to denote large changes. \n",
    "\n",
    "Implement this new physics of the world in the model, and design an encoding schedule that lets your model retrieve effectively (>7 success). The trivial solution is to shove all the encodings towards the back end of the encoding period, but doing so will increase your encoding load (proxy for study effort). \n",
    "\n",
    "An optimal solution would find the scheduling pattern that minimizes % encoding load while maintaining average retrieval success across multiple runs at 7. See if you can find it, assuming your retrieving agent knows the parameters of the model that will generate his world contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: 50 marks\n",
    "\n",
    "Do the same thing, but this time assuming that the retrieving agent does not know the parameters of the world generating model, just the fact that the rate of drift is sampled from a bimodal distribution. Hint: you'll have to have the agent learn the drift distribution parameters. EM is your friend here. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27f98c3cd29bdbbbcf21e34becd61d5d6f13f7e1fe4b4cefd3f8a1ca844857c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('cocosci')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
