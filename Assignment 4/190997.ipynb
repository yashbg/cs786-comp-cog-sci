{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "\\- Yash Gupta (190997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am giving you a dataset that contains 70 judgments a subject has made about the size of hypothetical people based on their weight (in kilos) and height (in inches). The subject has categorized people into three categories - small, average and large. \n",
    "\n",
    "The dataset data.csv contains the 70 actual judgments made by the subject as a 70x3 matrix. The first column contains weights, the second contains heights. The third column contains the category label assigned by the subject (small = 1, average = 2, large = 3). \n",
    "\n",
    "I am also giving you a test set test.csv of 10 more weight-height combinations as a 10x2 matrix (same column interpretations). I want you to tell me what a generalized context model would predict this subjects' category labels to be, assuming  \n",
    "(i) he is polite, and is far more likely to call someone big average than large  \n",
    "(ii) He is more likely to use weight than height to make category judgments about size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-barbados",
   "metadata": {},
   "source": [
    "### Q1. (30 points)\n",
    "\n",
    "Implement a GCM encoding these assumptions and give me quantitative predictions on the test set. Submit both code and category responses for the data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-barbados",
   "metadata": {},
   "source": [
    "### Q2. (40 points)\n",
    "\n",
    "I am also sharing with you, John McDonnell's python implementation of Anderson's Rational Model of Categorization (rational.py). Modify the code to obtain category predictions for the data I have shared with you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Anderson's venerable \"rational\" model of categorization.\n",
    "# Assumes that stimuli were generated by a mixture of Gaussian distributions;\n",
    "# rather than compute the full Bayesian posterior, it views items sequentially\n",
    "# and assigns each to the maximum a posteriori cluster.\n",
    "#\n",
    "# At the end it is presented with a stimulus with one item missing, and\n",
    "# predicts the probability that its value is a '0' or a '1'.\n",
    "#\n",
    "# Implemented in python by John McDonnell\n",
    "#\n",
    "# References: Anderson (1990) and Anderson (1991),\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "#Utility functions:\n",
    "\n",
    "class dLocalMAP:\n",
    "    \"\"\"\n",
    "    See Anderson (1990, 1991)\n",
    "    'Categories' renamed 'clusters' to avoid confusion.\n",
    "    Discrete version.\n",
    "    \n",
    "    Stimulus format is a list of integers from 0 to n-1 where n is the number\n",
    "    of possible features (e.g. [1,0,1])\n",
    "    \n",
    "    args: c, alphas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.partition = [[]]\n",
    "        self.c, self.alpha = args\n",
    "        self.alpha0 = sum(self.alpha.T)\n",
    "        self.N = 0\n",
    "    \n",
    "    def probClustVal(self, k, i, val):\n",
    "        \"\"\"Find P(j|k)\"\"\"\n",
    "        cj = len([x for x in self.partition[k] if x[i]==val])\n",
    "        nk = len(self.partition)\n",
    "        return (cj + self.alpha[i][val])/(nk + self.alpha0[i])\n",
    "    \n",
    "    def condclusterprob(self, stim, k):\n",
    "        \"\"\"Find P(F|k)\"\"\"\n",
    "        pjks = []\n",
    "        for i in range(len(stim)):\n",
    "            cj = len([x for x in self.partition[k] if x[i]==stim[i]])\n",
    "            nk = len(self.partition[k])\n",
    "            pjks.append( (cj + self.alpha[i][stim[i]])/(nk + self.alpha0[i]) )\n",
    "        return np.product( pjks )\n",
    "        \n",
    "    \n",
    "    def posterior(self, stim):\n",
    "        \"\"\"Find P(k|F) for each cluster\"\"\"\n",
    "        pk = np.zeros( len(self.partition) )\n",
    "        pFk = np.zeros( len(self.partition) )\n",
    "        \n",
    "        # existing clusters:\n",
    "        for k in range(len(self.partition)):\n",
    "            pk[k] = self.c * len(self.partition[k])/ ((1-self.c) + self.c * self.N)\n",
    "            if len(self.partition[k])==0: # case of new cluster\n",
    "                pk[k] = (1-self.c) / (( 1-self.c ) + self.c * self.N)\n",
    "            pFk[k] = self.condclusterprob( stim, k)\n",
    "        \n",
    "        # put it together\n",
    "        pkF = (pk*pFk) # / sum( pk*pFk )\n",
    "        \n",
    "        return pkF\n",
    "    \n",
    "    def stimulate(self, stim):\n",
    "        \"\"\"Argmax of P(k|F) + P(0|F)\"\"\"\n",
    "        winner = np.argmax( self.posterior(stim) )\n",
    "        print(\"Stim: \", stim)\n",
    "        print(\"Partition: \", self.partition)\n",
    "        print(self.posterior(stim))\n",
    "        \n",
    "        if len(self.partition[winner]) == 0:\n",
    "            self.partition.append( [] )\n",
    "        self.partition[winner].append(stim)\n",
    "        \n",
    "        self.N += 1\n",
    "    \n",
    "    def query(self, stimulus):\n",
    "        \"\"\"Queried value should be -1.\"\"\"\n",
    "        qdim = -1\n",
    "        for i in range(len(stimulus)):\n",
    "            if stimulus[i] < 0:\n",
    "                if qdim != -1:\n",
    "                    raise Exception(\"ERROR: Multiple dimensions queried.\")\n",
    "                qdim = i\n",
    "        \n",
    "        self.N = sum([len(x) for x in self.partition])\n",
    "        \n",
    "        pkF = self.posterior(stimulus)\n",
    "        pkF = pkF[:-1] / sum(pkF[:-1]) # eliminate `new cluster' prob\n",
    "        \n",
    "        pjF = np.array( [sum( [ pkF[k] * self.probClustVal(k, qdim, j) \\\n",
    "                for k in range(len(self.partition)-1)] ) \n",
    "                for j in range(len( self.alpha[qdim] ))] )\n",
    "        \n",
    "        return pjF / sum(pjF)\n",
    "\n",
    "def testlocalmapD():\n",
    "    \"\"\"\n",
    "    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.\n",
    "    \n",
    "    This script will print out the probability that each item belongs to each\n",
    "    of the existing clusters or to a new cluster, and the model assign it to\n",
    "    the most likely cluster. To see that the model is working correctly, you\n",
    "    can follow along with Anderson (1991), which steps through in the same way.\n",
    "    \"\"\"\n",
    "    stims = [[1, 1, 1, 1, 1], # Medin & Schaffer (1978)\n",
    "             [1, 0, 1, 0, 1], \n",
    "             [1, 0, 1, 1, 0],\n",
    "             [0, 0, 0, 0, 0],\n",
    "             [0, 1, 0, 1, 1],  \n",
    "             [0, 1, 0, 0, 0]]\n",
    "    # These are the classic Shepard Type II and Type IV datasets.\n",
    "    # Uncomment the one you want to try out; you might want to uncomment\n",
    "    # shuffling the stims too if you don't care about order.\n",
    "    #stims = [[0, 0, 0, 0], [0, 0, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [1, 0, 0, 0], [1, 0, 1, 1], [0, 1, 0, 0], [0, 1, 1, 1]] # Type IV\n",
    "    stims = [[0, 0, 0, 0], [0, 0, 1, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 0, 0, 1], [1, 0, 1, 1], [0, 1, 0, 1], [0, 1, 1, 1]] # Type II\n",
    "    \n",
    "    for _ in range(1):\n",
    "        model = dLocalMAP([.5, np.ones((len(stims[0]),2))])\n",
    "        \n",
    "        shuffle(stims)\n",
    "        for s in stims:\n",
    "            model.stimulate(s)\n",
    "        print(model.partition)\n",
    "        \n",
    "        print(\"Prob vals for 0,0,0,0,?\", model.query( [0]*(len(stims[0])-1) + [-1] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-barbados",
   "metadata": {},
   "source": [
    "### Q3. (30 points)\n",
    "\n",
    "For both GCM and RMC, show empirically using the dataset I've shared that both models assume exchangeability of data, viz. the order in which data enters the model does not affect the category labels of the model for any given subset of data. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27f98c3cd29bdbbbcf21e34becd61d5d6f13f7e1fe4b4cefd3f8a1ca844857c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('cocosci')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
